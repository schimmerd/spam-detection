{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam classifier with Naive Bayes (BoW & TF-IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from itertools import islice, chain\n",
    "from collections import defaultdict\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMS messages: 5569\n"
     ]
    }
   ],
   "source": [
    "path = \"data/SMSSpamCollection.csv\"\n",
    "\n",
    "ps = PorterStemmer()\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    cleaned_text = text.replace('\\W+', '').replace('\\s+', '').strip()\n",
    "    # lower\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    # tokenize\n",
    "    tokenized_text = cleaned_text.split()\n",
    "    # stopwords\n",
    "    stopwords_removed = [token for token in tokenized_text if token not in sw]\n",
    "    # stemming\n",
    "    stemmed_text = [ps.stem(token) for token in stopwords_removed]\n",
    "    punc_removed = [token.translate(str.maketrans('', '', string.punctuation)) for token in stemmed_text]\n",
    "    return [token for token in punc_removed if token != '']\n",
    "\n",
    "\n",
    "def read_sms_data(file_path):\n",
    "    sms_data = list()\n",
    "    with open(file_path, \"r\") as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile, fieldnames=[\"label\", \"sms\"], dialect=\"excel-tab\")\n",
    "        for row in csv_reader.reader:\n",
    "            # replace label with 0 (ham) or 1 (spam)\n",
    "            label = 1 if row[0].lower() == \"spam\" else 0\n",
    "            sms = row[1]\n",
    "            cleaned_sms = preprocess(sms)\n",
    "            if len(cleaned_sms) != 0:\n",
    "                sms_data.append(dict(label=label, sms=cleaned_sms))\n",
    "    return sms_data\n",
    "\n",
    "sms_data_list = read_sms_data(path)\n",
    "print(\"SMS messages:\", len(sms_data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split sms list into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 4455\n",
      "Size of testing set: 1114\n"
     ]
    }
   ],
   "source": [
    "def test_and_train_split(data, test_size=0.2, train_size=0.8, shuffle=True):\n",
    "    data_ = data\n",
    "    if shuffle:\n",
    "        data_ = random.sample(data, len(data))\n",
    "    train_and_test = [\n",
    "        list(\n",
    "            islice(iter(data_), elm)) for elm in [\n",
    "            round((len(data) * train_size)),\n",
    "            round((len(data) * test_size))\n",
    "        ]\n",
    "    ]\n",
    "    x_train = train_and_test[0]\n",
    "    x_test = list(map(lambda x: x.get(\"sms\"), train_and_test[1]))\n",
    "    y_test_label = list(map(lambda x: x.get(\"label\"), train_and_test[1]))\n",
    "    return x_train, x_test, y_test_label\n",
    "\n",
    "X_train, X_test, y_test = test_and_train_split(sms_data_list)\n",
    "\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Size of testing set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Ham messages in train: 3850\n",
      "Total number of Spam messages in train: 605\n"
     ]
    }
   ],
   "source": [
    "X_train_ham = list(map(lambda y: y.get(\"sms\"), filter(lambda x: x.get(\"label\") == 0, X_train)))\n",
    "X_train_spam = list(map(lambda y: y.get(\"sms\"), filter(lambda x: x.get(\"label\") == 1, X_train)))\n",
    "\n",
    "print(\"Total number of Ham messages in train:\", len(X_train_ham))\n",
    "print(\"Total number of Spam messages in train:\", len(X_train_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(vocab):\n",
    "    index_dict = dict()\n",
    "    i = 0\n",
    "    for word in vocab:\n",
    "        index_dict[word] = i\n",
    "        i += 1\n",
    "    return index_dict\n",
    "\n",
    "\n",
    "def counter(sentences, vocab):\n",
    "    count_dict = dict()\n",
    "    for word in vocab:\n",
    "        count_dict[word] = 0\n",
    "        for sent in sentences:\n",
    "            if word in sent:\n",
    "                count_dict[word] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of ham vocab: 6266\n",
      "Size of spam vocab: 2592\n"
     ]
    }
   ],
   "source": [
    "vocab_train_spam = set(list(chain.from_iterable(X_train_spam)))\n",
    "vocab_train_ham = set(list(chain.from_iterable(X_train_ham)))\n",
    "\n",
    "print(\"Size of ham vocab:\", len(vocab_train_ham))\n",
    "print(\"Size of spam vocab:\", len(vocab_train_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentences, index_word, vocab):\n",
    "    bow_list = list()\n",
    "    for sent in sentences:\n",
    "        count_dict = defaultdict(int)\n",
    "        vec = [float(0)] * len(vocab)\n",
    "        for word in sent:\n",
    "            count_dict[word] += 1.0\n",
    "        for word, count in count_dict.items():\n",
    "            vec[index_word[word]] = count\n",
    "        bow_list.append(vec)\n",
    "    return bow_list\n",
    "\n",
    "bow_index_spam = create_index(vocab_train_spam)\n",
    "bow_index_ham = create_index(vocab_train_ham)\n",
    "\n",
    "bow_vector_spam = bag_of_words(X_train_spam, bow_index_spam, vocab_train_spam)\n",
    "bow_vector_ham = bag_of_words(X_train_ham, bow_index_ham, vocab_train_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency\n",
    "def compute_tf(sentence, word):\n",
    "    N = len(sentence)\n",
    "    occ = len([token for token in sentence if token == word])\n",
    "    return occ / N\n",
    "\n",
    "\n",
    "# Inverse Document Frequency\n",
    "def compute_idf(word, count_dict, no_of_sentences):\n",
    "    try:\n",
    "        word_occ = count_dict[word] + 1\n",
    "    except KeyError:\n",
    "        word_occ = 1\n",
    "    return math.log(no_of_sentences / word_occ)\n",
    "\n",
    "\n",
    "# TF IDF combined\n",
    "def compute_tfidf(sentence, vocab, count_dict, index_dict, no_of_sentences):\n",
    "    tf_idf_vec = [float(0)] * len(vocab)\n",
    "    for word in sentence:\n",
    "        tf = compute_tf(sentence, word)\n",
    "        idf = compute_idf(word, count_dict, no_of_sentences)\n",
    "\n",
    "        value = tf * idf\n",
    "        tf_idf_vec[index_dict[word]] = value\n",
    "    return tf_idf_vec\n",
    "\n",
    "# Create word counts\n",
    "count_spam = counter(X_train_spam, vocab_train_spam)\n",
    "count_ham = counter(X_train_ham, vocab_train_ham)\n",
    "\n",
    "# Create index\n",
    "tfidf_index_spam = create_index(vocab_train_spam)\n",
    "tfidf_index_ham = create_index(vocab_train_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_vector(sentences, vocab, count_dict, index_dict):\n",
    "    vector_list = []\n",
    "    for sentence in sentences:\n",
    "        vec = compute_tfidf(sentence, vocab, count_dict, index_dict, len(sentences))\n",
    "        vector_list.append(vec)\n",
    "    return vector_list\n",
    "\n",
    "vector_spam = create_output_vector(X_train_spam, vocab_train_spam, count_spam, tfidf_index_spam)\n",
    "vector_ham = create_output_vector(X_train_ham, vocab_train_ham, count_ham, tfidf_index_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_spam = sum(list(chain.from_iterable(vector_spam)))\n",
    "sum_of_ham = sum(list(chain.from_iterable(vector_ham)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam = len(X_train_spam) / (len(X_train_spam) + (len(X_train_ham)))\n",
    "p_ham = len(X_train_ham) / (len(X_train_ham) + (len(X_train_spam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(message, method=\"tfidf\", verbose=False):\n",
    "    p_w_spam, p_w_ham = 0.0, 0.0\n",
    "    for word in message:\n",
    "\n",
    "        if method.lower() == \"tfidf\":\n",
    "            try:\n",
    "                p_w_spam += math.log((sum(list(vector[tfidf_index_spam[word]] for vector in vector_spam)) + 1) /  sum_of_spam + 2)\n",
    "            except KeyError:\n",
    "                p_w_spam += math.log(1 / (len(vector_spam) + 2)) #len(vector_ham) + 2))\n",
    "\n",
    "            try:\n",
    "                p_w_ham += math.log(sum(list(vector[tfidf_index_ham[word]] for vector in vector_ham)) + 1 / sum_of_ham + 2)\n",
    "            except KeyError:\n",
    "                p_w_ham += math.log(1 / (len(vector_ham) + 2)) #len(vector_ham) + 2))\n",
    "        else:\n",
    "            try:\n",
    "                p_w_spam += math.log((sum(list(vector[bow_index_spam[word]] for vector in bow_vector_spam)) + 1))\n",
    "            except KeyError:\n",
    "                p_w_spam += math.log(1 / (len(bow_vector_spam) + 2)) #len(bow_vector_ham) + 2))\n",
    "\n",
    "            try:\n",
    "                p_w_ham += math.log((sum(list(vector[bow_index_ham[word]] for vector in bow_vector_ham)) + 1))\n",
    "            except KeyError:\n",
    "                p_w_ham += math.log(1 / (len(bow_vector_ham) + 2)) #len(bow_vector_spam) + 2))\n",
    "\n",
    "        p_w_spam +=  math.log(p_spam)\n",
    "        p_w_spam += math.log(p_ham)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Model: {0}, spam score: {1}\".format(method.upper(), p_w_spam))\n",
    "        print(\"Model: {0}, ham score: {1}\".format(method.upper(), p_w_ham))\n",
    "\n",
    "    if p_w_spam >= p_w_ham:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, method):\n",
    "    result = dict()\n",
    "    for i, message in enumerate(test_data):\n",
    "        result[i] = classifier(message, method)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(matrix):\n",
    "    labels = [\"spam\", \"ham\"]\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    l = max(\n",
    "        reduce(lambda n, x : len(\"%s\"%x) if n < len(\"%s\"%x) else n, [0] + list(chain(matrix))),\n",
    "        reduce(lambda n, x : len(\"%s\"%x) if n < len(\"%s\"%x) else n, [0] + list(chain(labels)))\n",
    "    )\n",
    "\n",
    "    print(\"\\t\",eval(\"\\\"%%%is\\\"%%\\\"%s\\\"\"%(l, \"\")), end=\" \")\n",
    "\n",
    "    for column in labels:\n",
    "        print(eval(\"\\\"%%%is\\\"%%\\\"%s\\\"\"%(l, column)), end=\" \")\n",
    "    print()\n",
    "\n",
    "    i = -0\n",
    "    for row in matrix:\n",
    "        print(\"\\t\",eval(\"\\t\\\"%%%is\\\"%%\\\"%s\\\"\"%(l, labels[i] if i >= 0 else \"\" )), end=\" \")\n",
    "        i += 1\n",
    "        for column in row:\n",
    "            print(eval(\"\\\"%%%is\\\"%%%s\"%(l, column)), end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(labels, predictions):\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "    for i in range(len(labels)):\n",
    "        true_pos += int(labels[i] == 1 and predictions[i] == 1)\n",
    "        true_neg += int(labels[i] == 0 and predictions[i] == 0)\n",
    "        false_pos += int(labels[i] == 0 and predictions[i] == 1)\n",
    "        false_neg += int(labels[i] == 1 and predictions[i] == 0)\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    fall_out = false_pos / (false_pos + true_neg)\n",
    "    mcc = (true_pos * true_neg - false_pos * false_neg) / math.sqrt((true_pos + false_pos) * (true_pos + false_neg) * (true_neg + false_pos) * (true_neg + false_neg))\n",
    "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "\n",
    "    print()\n",
    "    print_matrix([[true_pos, false_neg], [false_pos, true_neg]])\n",
    "    print(\"\\nClassification report:\")\n",
    "    print()\n",
    "    print(\"Precision:\\t\", round(precision, 2))\n",
    "    print(\"Recall:\\t\\t\", round(recall, 2))\n",
    "    print(\"Fall-Out:\\t\", round(fall_out, 2))\n",
    "    print(\"MCC:\\t\\t\", round(mcc, 2))\n",
    "    print(\"Accuracy:\\t\", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict messages and print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_prediction = predict(X_test, \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test messages: 1114\n",
      "\n",
      "Confusion Matrix:\n",
      "\t              spam      ham \n",
      "\t     spam       94       63 \n",
      "\t      ham        0      957 \n",
      "\n",
      "Classification report:\n",
      "\n",
      "Precision:\t 1.0\n",
      "Recall:\t\t 0.6\n",
      "Fall-Out:\t 0.0\n",
      "MCC:\t\t 0.75\n",
      "Accuracy:\t 0.94\n"
     ]
    }
   ],
   "source": [
    "print(\"Total test messages:\", len(X_test))\n",
    "metrics(y_test, tfidf_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_prediction = predict(X_test, \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test messages: 1114\n",
      "\n",
      "Confusion Matrix:\n",
      "\t                spam       ham \n",
      "\t      spam       139        18 \n",
      "\t       ham         0       957 \n",
      "\n",
      "Classification report:\n",
      "\n",
      "Precision:\t 1.0\n",
      "Recall:\t\t 0.89\n",
      "Fall-Out:\t 0.0\n",
      "MCC:\t\t 0.93\n",
      "Accuracy:\t 0.98\n"
     ]
    }
   ],
   "source": [
    "print(\"Total test messages:\", len(X_test))\n",
    "metrics(y_test, bow_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
